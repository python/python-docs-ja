# SOME DESCRIPTIVE TITLE.
# Copyright (C) 1990-2018, Python Software Foundation
# This file is distributed under the same license as the Python package.
# 
# Translators:
# tomoğŸ§, 2017
# ç§˜æ¹¯ <xwhhsprings@gmail.com>, 2016
msgid ""
msgstr ""
"Project-Id-Version: Python 2.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-01-21 01:52+0900\n"
"PO-Revision-Date: 2017-09-22 17:56+0000\n"
"Last-Translator: tomoğŸ§\n"
"Language-Team: Japanese (http://www.transifex.com/python-doc/python-27/language/ja/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: ja\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../library/tokenize.rst:2
msgid ":mod:`tokenize` --- Tokenizer for Python source"
msgstr ":mod:`tokenize` --- Pythonã‚½ãƒ¼ã‚¹ã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶"

#: ../../library/tokenize.rst:9
msgid "**Source code:** :source:`Lib/tokenize.py`"
msgstr "**ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰:** :source:`Lib/tokenize.py`"

#: ../../library/tokenize.rst:13
msgid ""
"The :mod:`tokenize` module provides a lexical scanner for Python source "
"code, implemented in Python.  The scanner in this module returns comments as"
" tokens as well, making it useful for implementing \"pretty-printers,\" "
"including colorizers for on-screen displays."
msgstr ":mod:`tokenize` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯ã€Python ã§å®Ÿè£…ã•ã‚ŒãŸ Python ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®å­—å¥è§£æå™¨ã‚’æä¾›ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å­—å¥è§£æå™¨ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚‚ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚ã“ã®ãŸã‚ã€ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ä¸Šã§è¡¨ç¤ºã™ã‚‹éš›ã®è‰²ä»˜ã‘æ©Ÿèƒ½ (colorizers) ã‚’å«ã‚€ \"æ¸…æ›¸å‡ºåŠ›å™¨ (pretty-printer)\" ã‚’å®Ÿè£…ã™ã‚‹ä¸Šã§ä¾¿åˆ©ã§ã™ã€‚"

#: ../../library/tokenize.rst:18
msgid ""
"To simplify token stream handling, all :ref:`operators` and "
":ref:`delimiters` tokens are returned using the generic :data:`token.OP` "
"token type.  The exact type can be determined by checking the second field "
"(containing the actual token string matched) of the tuple returned from "
":func:`tokenize.generate_tokens` for the character sequence that identifies "
"a specific operator token."
msgstr "ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®æ‰±ã„ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã«ã€å…¨ã¦ã® :ref:`operators` ã¨ :ref:`delimiters` ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¸ã‚§ãƒãƒªãƒƒã‚¯ãª :data:`token.OP` ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚æ­£ç¢ºãªå‹ã¯ :func:`tokenize.generate_tokens` ã‹ã‚‰ã®æˆ»ã‚Šå€¤ã®ã‚¿ãƒ—ãƒ«ã® 2 ç•ªç›®ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ (å®Ÿéš›ã®ãƒãƒƒãƒã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã‚’å«ã‚“ã§ã„ã¾ã™) ã‚’ç‰¹å®šã®æ¼”ç®—å­ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è­˜åˆ¥ã™ã‚‹æ–‡å­—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚Œã°æ±ºå®šå‡ºæ¥ã¾ã™ã€‚"

#: ../../library/tokenize.rst:25
msgid "The primary entry point is a :term:`generator`:"
msgstr "ç¬¬ä¸€ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿(:term:`generator`)ã§ã™:"

#: ../../library/tokenize.rst:29
msgid ""
"The :func:`generate_tokens` generator requires one argument, *readline*, "
"which must be a callable object which provides the same interface as the "
":meth:`~file.readline` method of built-in file objects (see section :ref"
":`bltin-file-objects`).  Each call to the function should return one line of"
" input as a string. Alternately, *readline* may be a callable object that "
"signals completion by raising :exc:`StopIteration`."
msgstr ":func:`generate_tokens` ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã¯ä¸€ã¤ã®å¼•æ•° *readline* ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚ã“ã®å¼•æ•°ã¯å‘¼ã³å‡ºã—å¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã€çµ„ã¿è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãŠã‘ã‚‹ :meth:`~file.readline` ãƒ¡ã‚½ãƒƒãƒ‰ã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¦ã„ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ (:ref:`bltin-file-objects` ç¯€ã‚’å‚ç…§ã—ã¦ãã ã•ã„)ã€‚ã“ã®é–¢æ•°ã¯å‘¼ã³å‡ºã—ã®ãŸã³ã«å…¥åŠ›å†…ã®ä¸€è¡Œã‚’æ–‡å­—åˆ—ã§è¿”ã•ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ã‚‚ã—ãã¯ã€ *readline* ã‚’ :exc:`StopIteration` ã‚’é€å‡ºã™ã‚‹ã“ã¨ã§å®Œäº†ã‚’çŸ¥ã‚‰ã›ã‚‹å‘¼ã³å‡ºã—å¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"

#: ../../library/tokenize.rst:36
msgid ""
"The generator produces 5-tuples with these members: the token type; the "
"token string; a 2-tuple ``(srow, scol)`` of ints specifying the row and "
"column where the token begins in the source; a 2-tuple ``(erow, ecol)`` of "
"ints specifying the row and column where the token ends in the source; and "
"the line on which the token was found.  The line passed (the last tuple "
"item) is the *logical* line; continuation lines are included."
msgstr "ã“ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã¯æ¬¡ã® 5 è¦ç´ ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã—ã¾ã™; ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—; ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—; ã‚½ãƒ¼ã‚¹å†…ã§ãã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒã©ã®è¡Œã€åˆ—ã§é–‹å§‹ã™ã‚‹ã‹ã‚’ç¤ºã™ int ã® ``(srow, scol)`` ã‚¿ãƒ—ãƒ«; ã©ã®è¡Œã€åˆ—ã§çµ‚äº†ã™ã‚‹ã‹ã‚’ç¤ºã™ int ã® ``(erow, ecol)`` ã‚¿ãƒ—ãƒ«; ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸè¡Œã€‚ (ã‚¿ãƒ—ãƒ«ã®æœ€å¾Œã®è¦ç´ ã«ã‚ã‚‹) è¡Œã¯ *è«–ç†* è¡Œã§ã€ç¶™ç¶šè¡ŒãŒå«ã¾ã‚Œã¾ã™ã€‚"

#: ../../library/tokenize.rst:45
msgid "An older entry point is retained for backward compatibility:"
msgstr "å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«å¤ã„ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆãŒæ®‹ã•ã‚Œã¦ã„ã¾ã™:"

#: ../../library/tokenize.rst:50
msgid ""
"The :func:`.tokenize` function accepts two parameters: one representing the "
"input stream, and one providing an output mechanism for :func:`.tokenize`."
msgstr ":func:`.tokenize` é–¢æ•°ã¯äºŒã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–ã‚Šã¾ã™: ä¸€ã¤ã¯å…¥åŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¡¨ã—ã€ã‚‚ã†ä¸€ã¤ã¯ :func:`.tokenize` ã®ãŸã‚ã®å‡ºåŠ›ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä¸ãˆã¾ã™ã€‚"

#: ../../library/tokenize.rst:53
msgid ""
"The first parameter, *readline*, must be a callable object which provides "
"the same interface as the :meth:`~file.readline` method of built-in file "
"objects (see section :ref:`bltin-file-objects`).  Each call to the function "
"should return one line of input as a string. Alternately, *readline* may be "
"a callable object that signals completion by raising :exc:`StopIteration`."
msgstr "æœ€åˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ *readline* ã¯ã€çµ„ã¿è¾¼ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã® :meth:`~file.readline` ãƒ¡ã‚½ãƒƒãƒ‰ã¨åŒã˜ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ã‚¤ã‚¹ã‚’æä¾›ã™ã‚‹å‘¼ã³å‡ºã—å¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ ( :ref:`bltin-file-objects` ç¯€ã‚’å‚ç…§)ã€‚ã“ã®é–¢æ•°ã¯å‘¼ã³å‡ºã—ã®ãŸã³ã«å…¥åŠ›å†…ã®ä¸€è¡Œã‚’æ–‡å­—åˆ—ã§è¿”ã•ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ã‚‚ã—ãã¯ã€ *readline* ã‚’ :exc:`StopIteration` ã‚’é€å‡ºã™ã‚‹ã“ã¨ã§å®Œäº†ã‚’çŸ¥ã‚‰ã›ã‚‹å‘¼ã³å‡ºã—å¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"

#: ../../library/tokenize.rst:59
msgid "Added :exc:`StopIteration` support."
msgstr ":exc:`StopIteration` ã‚µãƒãƒ¼ãƒˆã®è¿½åŠ ã€‚"

#: ../../library/tokenize.rst:62
msgid ""
"The second parameter, *tokeneater*, must also be a callable object.  It is "
"called once for each token, with five arguments, corresponding to the tuples"
" generated by :func:`generate_tokens`."
msgstr "äºŒç•ªç›®ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ *tokeneater* ã‚‚å‘¼ã³å‡ºã—å¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ã“ã®é–¢æ•°ã¯å„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ä¸€åº¦ã ã‘å‘¼ã³å‡ºã•ã‚Œã€ :func:`generate_tokens` ãŒç”Ÿæˆã™ã‚‹ã‚¿ãƒ—ãƒ«ã«å¯¾å¿œã™ã‚‹ 5 ã¤ã®å¼•æ•°ã‚’ã¨ã‚Šã¾ã™ã€‚"

#: ../../library/tokenize.rst:66
msgid ""
"All constants from the :mod:`token` module are also exported from "
":mod:`tokenize`, as are two additional token type values that might be "
"passed to the *tokeneater* function by :func:`.tokenize`:"
msgstr ":mod:`token` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…¨ã¦ã®å®šæ•°ã¯ :mod:`tokenize` ã§ã‚‚å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã«åŠ ãˆã€ä»¥ä¸‹ã®äºŒã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³å€¤ãŒ :func:`.tokenize` ã® *tokeneater* é–¢æ•°ã«æ¸¡ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:"

#: ../../library/tokenize.rst:73
msgid "Token value used to indicate a comment."
msgstr "ã‚³ãƒ¡ãƒ³ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’è¡¨ã™ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³å€¤ã§ã™ã€‚"

#: ../../library/tokenize.rst:78
msgid ""
"Token value used to indicate a non-terminating newline.  The NEWLINE token "
"indicates the end of a logical line of Python code; NL tokens are generated "
"when a logical line of code is continued over multiple physical lines."
msgstr "çµ‚ã‚ã‚Šã§ã¯ãªã„æ”¹è¡Œã‚’è¡¨ã™ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³å€¤ã€‚NEWLINE ãƒˆãƒ¼ã‚¯ãƒ³ã¯ Pythonã‚³ãƒ¼ãƒ‰ã®è«–ç†è¡Œã®çµ‚ã‚ã‚Šã‚’è¡¨ã—ã¾ã™ã€‚NLãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚³ãƒ¼ãƒ‰ã®è«–ç†è¡ŒãŒè¤‡æ•°ã®ç‰©ç†è¡Œã«ã‚ãŸã£ã¦ç¶šã„ã¦ã„ã‚‹ã¨ãã«ä½œã‚‰ã‚Œã¾ã™ã€‚"

#: ../../library/tokenize.rst:82
msgid ""
"Another function is provided to reverse the tokenization process. This is "
"useful for creating tools that tokenize a script, modify the token stream, "
"and write back the modified script."
msgstr "ã‚‚ã†ä¸€ã¤ã®é–¢æ•°ãŒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€†è»¢ã™ã‚‹ãŸã‚ã«æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å­—å¥è§£æã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å¤‰æ›´ã‚’åŠ ãˆã€å¤‰æ›´ã•ã‚ŒãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æ›¸ãæˆ»ã™ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹éš›ã«ä¾¿åˆ©ã§ã™ã€‚"

#: ../../library/tokenize.rst:89
msgid ""
"Converts tokens back into Python source code.  The *iterable* must return "
"sequences with at least two elements, the token type and the token string.  "
"Any additional sequence elements are ignored."
msgstr "ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ—ã‚’ Python ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¾ã™ã€‚ *iterable* ã¯å°‘ãªãã¨ã‚‚äºŒã¤ã®è¦ç´ ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—ãŠã‚ˆã³ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã€ã‹ã‚‰ãªã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è¿”ã•ãªã‘ã‚Œã°ã„ã‘ã¾ã›ã‚“ã€‚ ãã®ä»–ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®è¦ç´ ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚"

#: ../../library/tokenize.rst:93
msgid ""
"The reconstructed script is returned as a single string.  The result is "
"guaranteed to tokenize back to match the input so that the conversion is "
"lossless and round-trips are assured.  The guarantee applies only to the "
"token type and token string as the spacing between tokens (column positions)"
" may change."
msgstr "å†æ§‹ç¯‰ã•ã‚ŒãŸã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä¸€ã¤ã®æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚å¾—ã‚‰ã‚Œã‚‹çµæœã¯ã‚‚ã†ä¸€åº¦å­—å¥è§£æã™ã‚‹ã¨å…¥åŠ›ã¨ä¸€è‡´ã™ã‚‹ã“ã¨ãŒä¿è¨¼ã•ã‚Œã‚‹ã®ã§ã€å¤‰æ›ãŒãƒ­ã‚¹ãƒ¬ã‚¹ã§ã‚ã‚Šãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆãƒªãƒƒãƒ—ã§ãã‚‹ã“ã¨ã¯é–“é•ã„ã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã®ä¿è¨¼ã¯ãƒˆãƒ¼ã‚¯ãƒ³å‹ãŠã‚ˆã³ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã«å¯¾ã—ã¦ã®ã‚‚ã®ã§ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ã‚¹ãƒšãƒ¼ã‚¹ (ã‚³ãƒ©ãƒ ä½ç½®)ã®ã‚ˆã†ãªã‚‚ã®ã¯å¤‰ã‚ã‚‹ã“ã¨ãŒã‚ã‚Šå¾—ã¾ã™ã€‚"

#: ../../library/tokenize.rst:103
msgid ""
"Raised when either a docstring or expression that may be split over several "
"lines is not completed anywhere in the file, for example::"
msgstr "docstring ã‚„è¤‡æ•°è¡Œã«ã‚ãŸã‚‹ã“ã¨ãŒè¨±ã•ã‚Œã‚‹å¼ãŒãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ã©ã“ã‹ã§çµ‚ã‚ã£ã¦ã„ãªã„å ´åˆã«é€å‡ºã•ã‚Œã¾ã™ã€‚ä¾‹ãˆã°::"

#: ../../library/tokenize.rst:109
msgid "or::"
msgstr "ã‚‚ã—ãã¯::"

#: ../../library/tokenize.rst:115
msgid ""
"Note that unclosed single-quoted strings do not cause an error to be raised."
" They are tokenized as ``ERRORTOKEN``, followed by the tokenization of their"
" contents."
msgstr "é–‰ã˜ã¦ã„ãªã„ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆæ–‡å­—åˆ—ã¯é€å‡ºã•ã‚Œã‚‹ã¹ãã‚¨ãƒ©ãƒ¼ã®åŸå› ã¨ãªã‚‰ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãã‚Œã‚‰ã¯ ``ERRORTOKEN`` ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã€ç¶šã„ã¦ãã®å†…å®¹ãŒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã¾ã™ã€‚"

#: ../../library/tokenize.rst:119
msgid ""
"Example of a script re-writer that transforms float literals into Decimal "
"objects::"
msgstr "ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›¸ãæ›ãˆã®ä¾‹ã§ã€æµ®å‹•å°æ•°ç‚¹æ•°ãƒªãƒ†ãƒ©ãƒ«ã‚’ Decimal ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã¾ã™::"
