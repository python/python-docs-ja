# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2001-2018, Python Software Foundation
# This file is distributed under the same license as the Python package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
# 
# Translators:
# tomoğŸ§, 2018
# 
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Python 3.6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-01-05 15:01+0000\n"
"PO-Revision-Date: 2018-06-29 17:30+0000\n"
"Last-Translator: tomoğŸ§, 2018\n"
"Language-Team: Japanese (https://www.transifex.com/python-doc/teams/5390/ja/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: ja\n"
"Plural-Forms: nplurals=1; plural=0;\n"

#: ../../library/urllib.robotparser.rst:2
msgid ":mod:`urllib.robotparser` ---  Parser for robots.txt"
msgstr ":mod:`urllib.robotparser` ---  robots.txt ã®ãŸã‚ã®ãƒ‘ãƒ¼ã‚¶"

#: ../../library/urllib.robotparser.rst:10
msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰:** :source:`Lib/urllib/robotparser.py`"

#: ../../library/urllib.robotparser.rst:20
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers"
" questions about whether or not a particular user agent can fetch a URL on "
"the Web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see "
"http://www.robotstxt.org/orig.html."
msgstr ""
"ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã¯å˜ä¸€ã®ã‚¯ãƒ©ã‚¹ã€ :class:`RobotFileParser` ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã®ã‚¯ãƒ©ã‚¹ã¯ã€ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ "
":file:`robots.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¬é–‹ã—ã¦ã„ã‚‹ Web ã‚µã‚¤ãƒˆã®ã‚ã‚‹ URL ã‚’å–å¾—å¯èƒ½ã‹ã©ã†ã‹ã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚ "
":file:`robots.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ ã«é–¢ã™ã‚‹è©³ç´°ã¯ http://www.robotstxt.org/orig.html "
"ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../library/urllib.robotparser.rst:28
msgid ""
"This class provides methods to read, parse and answer questions about the "
":file:`robots.txt` file at *url*."
msgstr "*url* ã® :file:`robots.txt` ã«å¯¾ã—èª­ã¿è¾¼ã¿ã€ãƒ‘ãƒ¼ã‚ºã€å¿œç­”ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:33
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr ":file:`robots.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã® URL ã‚’è¨­å®šã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:37
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr ":file:`robots.txt` URL ã‚’èª­ã¿å‡ºã—ã€ãƒ‘ãƒ¼ã‚¶ã«å…¥åŠ›ã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:41
msgid "Parses the lines argument."
msgstr "å¼•æ•° *lines* ã®å†…å®¹ã‚’è§£é‡ˆã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:45
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"è§£é‡ˆã•ã‚ŒãŸ :file:`robots.txt` ãƒ•ã‚¡ã‚¤ãƒ«ä¸­ã«è¨˜è¼‰ã•ã‚ŒãŸè¦å‰‡ã«å¾“ã£ãŸã¨ãã€ *useragent* ãŒ *url* "
"ã‚’å–å¾—ã—ã¦ã‚‚ã‚ˆã„å ´åˆã«ã¯ ``True`` ã‚’è¿”ã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:51
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files"
" periodically."
msgstr ""
"``robots.txt`` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å¾Œã«å–å¾—ã—ãŸæ™‚åˆ»ã‚’è¿”ã—ã¾ã™ã€‚ã“ã®å€¤ã¯ã€å®šæœŸçš„ã«æ–°ãŸãª ``robots.txt`` "
"ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€é•·æ™‚é–“å‹•ä½œã™ã‚‹ Web ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè£…ã™ã‚‹éš›ã«ä¾¿åˆ©ã§ã™ã€‚"

#: ../../library/urllib.robotparser.rst:57
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr "``robots.txt`` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æœ€å¾Œã«å–å¾—ã—ãŸæ™‚åˆ»ã‚’ç¾åœ¨ã®æ™‚åˆ»ã«è¨­å®šã—ã¾ã™ã€‚"

#: ../../library/urllib.robotparser.rst:62
msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` for "
"the *useragent* in question.  If there is no such parameter or it doesn't "
"apply to the *useragent* specified or the ``robots.txt`` entry for this "
"parameter has invalid syntax, return ``None``."
msgstr ""

#: ../../library/urllib.robotparser.rst:71
msgid ""
"Returns the contents of the ``Request-rate`` parameter from ``robots.txt`` "
"as a :term:`named tuple` ``RequestRate(requests, seconds)``. If there is no "
"such parameter or it doesn't apply to the *useragent* specified or the "
"``robots.txt`` entry for this parameter has invalid syntax, return ``None``."
msgstr ""

#: ../../library/urllib.robotparser.rst:80
msgid ""
"The following example demonstrates basic use of the :class:`RobotFileParser`"
" class::"
msgstr ""
